import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st
from pyhelpers.store import load_pickle
from streamlit_yellowbrick import st_yellowbrick
import warnings
warnings.filterwarnings('ignore')
import plotly.express as px
import squarify
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from feature_engine.wrappers import SklearnTransformerWrapper
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from scipy import stats
from pyclustertend import hopkins, ivat
from sklearn.decomposition import PCA
import pickle
import io
from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC, roc_auc

#--------------
#Function
#--------------

#--------------
#LOAD V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU
#--------------
#LOAD d·ªØ li·ªáu raw
df = pd.read_fwf('CDNOW_master.txt',header = None, names = ['Customer_id','Dates','Net_quantity','Total_sales'])
df['Dates'] = pd.to_datetime(df['Dates'], format='%Y%m%d')

##Chu·∫©n b·ªã df df_recency
df_recency = df.groupby(by='Customer_id',
                        as_index=False)['Dates'].max()
df_recency.columns = ['Customer_id', 'LastPurchaseDate']
recent_date = df_recency['LastPurchaseDate'].max()
df_recency['Recency'] = df_recency['LastPurchaseDate'].apply(
    lambda x: (recent_date - x).days)

##Chu·∫©n b·ªã df frequency_df
frequency_df = df.groupby(by=['Customer_id'], as_index=False)['Dates'].count()
frequency_df.columns = ['Customer_id', 'Frequency']

##Chu·∫©n b·ªã df frequency_df
monetary_df = df.groupby(by='Customer_id', as_index=False)['Total_sales'].sum()
monetary_df.columns = ['Customer_id', 'Monetary']

##Chu·∫©n b·ªã dataframe RFM
rf_df = df_recency.merge(frequency_df, on='Customer_id')
rfm_df = rf_df.merge(monetary_df, on='Customer_id').drop(
    columns='LastPurchaseDate')

##Create labels for Recency, Frequency, Monetary
r_labels = range(4, 0, -1)  
f_labels = range(1, 5)
m_labels = range(1, 5)
##Assign these labels to 4 equal percentile groups
r_groups = pd.qcut(rfm_df['Recency'].rank(method='first'), q=4, labels=r_labels)     
f_groups = pd.qcut(rfm_df['Frequency'].rank(method='first'), q=4, labels=f_labels)
m_groups = pd.qcut(rfm_df['Monetary'].rank(method='first'), q=4, labels=m_labels)
# Create new columns R, F, M
RFM = rfm_df.assign(R = r_groups.values, F = f_groups.values, M = m_groups.values)
rfm_df_2 = RFM.copy()

#Concat RFM quartile values to create RFM Segment
def join_rfm(x): return str(int(x['R'])) + str(int(x['F'])) + str(int(x['M']))
rfm_df_2['RFM_Segment'] = rfm_df_2.apply(join_rfm, axis=1)
rfm_df_3 = rfm_df_2.copy()
##Count num of unique segments
rfm_count_unique = rfm_df_2.groupby('RFM_Segment')['RFM_Segment'].nunique()
rfm_count_unique.sum()

#Calculate RFM score and level and Manual Segmentation
rfm_df_3['RFM_Score'] = rfm_df_3[['R','F','M']].sum(axis=1)

def rfm_level(df):
    if (df['RFM_Score'] == 12) :
        return 'STARS'  
    elif (df['R'] == 4 and df['F'] ==1 and df['M'] == 1):
        return 'NEW'
    else:     
        if df['M'] == 4:
            return 'BIG SPENDER'
        elif df['F'] == 4:
            return 'LOYAL'
        elif df['R'] == 4:
            return 'ACTIVE'
        elif df['R'] == 1:
            return 'LOST'
        elif df['M'] == 1:
            return 'LIGHT'
        return 'REGULARS'

rfm_df_3['RFM_level'] = rfm_df_3.apply(rfm_level, axis=1)

## Calculate mean values for each segments
rfm_agg = rfm_df_3.groupby('RFM_level').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': ['mean', 'count']}).round(0)

rfm_agg.columns = rfm_agg.columns.droplevel()
rfm_agg.columns = ['RecencyMean','FrequencyMean','MonetaryMean', 'Count']
rfm_agg['Percent'] = round((rfm_agg['Count']/rfm_agg.Count.sum())*100, 2)

# Reset the index
rfm_agg = rfm_agg.reset_index()

#Create our plot and resize it.
# fig = plt.gcf()
# ax = fig.add_subplot()
# fig.set_size_inches(8, 10)
# colors_dict = {'ACTIVE':'aliceblue','BIG SPENDER':'royalblue', 'LIGHT':'cyan',
#                'LOST':'red', 'LOYAL':'purple', 'POTENTIAL':'green', 'STARS':'gold'}
# squarify.plot(sizes=rfm_agg['Count'],
#               text_kwargs={'fontsize':14,'weight':'bold', 'fontname':"sans serif"},
#               color=colors_dict.values(),
#               label=['{} \n{:.0f} days \n{:.0f} orders \n{:.0f} $ \n{:.0f} customers ({}%)'.format(*rfm_agg.iloc[i])
#                       for i in range(0, len(rfm_agg))], alpha=0.5 )
# plt.title("Customers Segments",fontsize=26,fontweight="bold")
# plt.axis('off')
# plt.savefig('RFM Segments.png')
# plt.show()

#Prepare for new df which remove outliers
new_df = RFM[['Customer_id','Recency','Frequency','Monetary']].copy()

# remove outliers
z_scores = stats.zscore(new_df)
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
new_df = new_df[filtered_entries]

#Scaling data
def preprocess(df):
    """Preprocess data for model clustering"""
    df_log = np.log1p(df)
    scaler = StandardScaler()
    scaler.fit(df_log)
    norm = scaler.transform(df_log)
    return norm

df_scale = preprocess(new_df)

# hopkins(new_df, new_df.shape[0]) 
#feature selection with PCA
pca = PCA(n_components=3)
principalComponents = pca.fit_transform(df_scale)
features = range(pca.n_components_)
PCA_components = pd.DataFrame(principalComponents)

#kmean
ks = range(1,10)
inertias=[]
for k in ks :
    # Create a KMeans clusters
    kmean = KMeans(n_clusters=k,random_state=1)
    kmean.fit(PCA_components.iloc[:,:1])
    inertias.append(kmean.inertia_)

# clustering
kmean1 = KMeans(n_clusters= 3, random_state=42)
kmean1.fit(PCA_components.iloc[:,:1])
cluster_labels = kmean1.labels_
rfm_rfm_k = new_df.assign(K_Cluster = cluster_labels)

km_pca = rfm_rfm_k.groupby('K_Cluster').agg({'Recency': 'mean','Frequency': 'mean',
                                         'Monetary': ['mean', 'count'],}).round(0)

rfm_rfm_k2 = rfm_rfm_k.copy()
rfm_rfm_k2["Segment"]=rfm_rfm_k2['K_Cluster'].map(lambda x:"STAR" if x ==1
                                              else "HIDDEN GEM" if x == 2 
                                              else "ROCK")
#save result and model
rfm_rfm_k.to_csv("result_KMeans.csv")
import dill
with open("KMeans.pkl", 'wb') as file:  
    dill.dump(kmean1, file)

#Calculate average values for each RFM_Level, and return a size of each segment 
rfm_agg2 = rfm_rfm_k2.groupby('Segment').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'Monetary': ['mean', 'count']}).round(1)
rfm_agg2.columns = rfm_agg2.columns.droplevel()
rfm_agg2.columns = ['RecencyMean','FrequencyMean','MonetaryMean', 'Count']
rfm_agg2['Percent'] = round((rfm_agg2['Count']/rfm_agg2.Count.sum())*100, 2)
# Reset the index
rfm_agg2 = rfm_agg2.reset_index()
silhouette_score(PCA_components.iloc[:,:1], kmean1.labels_, metric='euclidean')

#Create tree plot
#fig_2 = plt.gcf()
#ax2 = fig_2.add_subplot()
#fig_2.set_size_inches(8, 10)
#colors_dict_2 = {'GOLD':'gold','HIDDEN GEM':'cyan','Promising':'red'}
#squarify.plot(sizes=rfm_agg2['Count'],
#            text_kwargs={'fontsize':12,'weight':'bold', 'fontname':"sans serif"},
#            color=colors_dict_2.values(),
#            label=['{} \n{:.0f} days \n{:.0f} orders \n{:.0f} $ \n{:.0f} customers ({}%)'.format(*rfm_agg2.iloc[i])
#                    for i in range(0, len(rfm_agg2))], alpha=0.5 )
#plt.title("Customers Segments",fontsize=26,fontweight="bold")
#plt.axis('off')
#plt.savefig('Kmean_Segments.png')
#plt.show()

#--------------
#LOAD MODEL ƒê√É BUILD 
#--------------
with open('KMeans.pkl', 'rb') as model_file:
    phanloai_khachhang = pickle.load(model_file)

#--------------
#FUNCTION Ch·∫°y ph·∫ßn 2
#--------------
def run_all(rfm_df):
    # 1. Show data
    st.subheader('1. Th√¥ng tin b·ªô d·ªØ li·ªáu')
    st.write('5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu')
    st.dataframe(rfm_df.head(5))
    st.write('5 d√≤ng d·ªØ li·ªáu cu·ªëi')
    st.dataframe(rfm_df.tail(5))
    st.write('K√≠ch th∆∞·ªõc d·ªØ li·ªáu')
    st.code('S·ªë d√≤ng: '+str(rfm_df.shape[0]) + ' v√† s·ªë c·ªôt: '+ str(rfm_df.shape[1]))
    n_null = rfm_df.isnull().any().sum()
    st.code('S·ªë d√≤ng b·ªã NaN: '+ str(n_null))
   
    st.subheader('Th·ªëng k√™ d·ªØ li·ªáu')
    st.dataframe(rfm_df.describe())    

    # 2. Tr·ª±c quan h√≥a d·ªØ li·ªáu
    st.subheader('2. EDA')
    fig_plot = plt.figure(figsize=(8,10))
    plt.subplot(3, 1, 1)
    sns.distplot(rfm_df['Recency'])# Plot distribution of R
    plt.subplot(3, 1, 2)
    sns.distplot(rfm_df['Frequency'])# Plot distribution of F
    plt.subplot(3, 1, 3)
    sns.distplot(rfm_df['Monetary']) # Plot distribution of M
    st.pyplot(fig_plot)

    st.write('''
    #### D·ª±a v√†o bi·ªÉu ƒë·ªì ph√¢n ph·ªëi:
    * Ph√¢n ph·ªëi c·ªßa Recency b·ªã l·ªách tr√°i
    * Ph√¢n ph·ªëi c·ªßa Frequency v√† Monetary l·ªách ph·∫£i
    ''')
    # 3. S·ª≠ d·ª•ng quartiles ƒë·ªÉ t√≠nh RFM 
    st.subheader('3. S·ª≠ d·ª•ng quartiles ƒë·ªÉ t√≠nh RFM')
    st.write('Create labels for Recency, Frequency, Monetary')
    st.code('''r_labels = range(4, 0, -1)
    
f_labels = range(1, 5)
    
m_labels = range(1, 5)  
    ''')
    st.write('Assign these labels to 4 equal percentile groups')
    st.code('''r_groups = pd.qcut(rfm_df['Recency'].rank(method='first'), q=4, labels=r_labels) 
        
f_groups = pd.qcut(rfm_df['Frequency'].rank(method='first'), q=4, labels=f_labels)
        
m_groups = pd.qcut(rfm_df['Monetary'].rank(method='first'), q=4, labels=m_labels)
    ''') 
    st.write('Create new columns R, F, M')
    st.code('''RFM = rfm_df.assign(R = r_groups.values, F = f_groups.values, M = m_groups.values)
    ''')
    st.write('5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu RFM sau khi ƒë∆∞·ª£c t·∫°o')
    st.dataframe(RFM.head(5))

    st.write('Concat RFM quartile values to create RFM Segment')
    st.dataframe(rfm_df_2.head())
    st.code('S·ªë l∆∞·ª£ng unique segment l√†' + ' ' + str(rfm_count_unique.sum()))
    st.write('')
    # 4. Calculate RFM score and level and Manual Segmentation. 
    st.subheader('4. T√≠nh RFM score, level v√† Manual Segmentation')
    st.write('D·ªØ li·ªáu sau khi t√≠nh to√°n')
    st.dataframe(rfm_df_3.head())
    st.write('Mean & t·ª∑ tr·ªçng c·ªßa c√°c segments sau khi l√†m Manual Segmentation')
    st.dataframe(rfm_agg)
    st.write('Treemap')
    st.image("RFM_Segments_manual.png", caption='Manual Segmentation')
    st.write('Scatter Plot (RFM)')
    pig_dot = px.scatter(rfm_agg, x="RecencyMean", y="MonetaryMean", size="FrequencyMean", color="RFM_level",
           hover_name="RFM_level", size_max=50)
    st.plotly_chart(pig_dot)
    st.write('''
    Nh·∫≠n x√©t:
    Nh√¨n 2 bi·ªÉu ƒë·ªì ta c√≥ th·ªÉ th·∫•y k·∫øt qu·∫£ ph√¢n c·ª•m c·ªßa ph∆∞∆°ng ph√°p v·∫´n ch∆∞a ƒë∆∞·ª£c t·ªët v√¨ c√°c nh√≥m c√≤n tr√πm nhau nhi·ªÅu.
    V√¨ v·∫≠y ta n√™n d√πng ph∆∞∆°ng ph√°p kh√°c ƒë·ªÉ ph√¢n c·ª•m l·∫°i b·ªô d·ªØ li·ªáu kh√°ch h√†ng s·∫µn c√≥
    ''')
    # 5. Data Pre-Processing
    st.subheader('5. Data Pre-Processing')
    #Outliers
    st.write('Bi·ªÉu ƒë·ªì boxplot')
    fig_box = plt.figure(figsize=(8, 10))
    plt.subplot(3, 1, 1)
    sns.boxplot(x = rfm_df['Recency'])
    plt.subplot(3, 1, 2)
    sns.boxplot(x = rfm_df['Frequency'])
    plt.subplot(3, 1, 3)
    sns.boxplot(x = rfm_df['Monetary'])
    st.pyplot(fig_box)
    st.write('Lo·∫°i outliers')
    st.code('''# remove outliers
    z_scores = stats.zscore(new_df)
    abs_z_scores = np.abs(z_scores)
    filtered_entries = (abs_z_scores < 3).all(axis=1)
    new_df = new_df[filtered_entries]
    ''')
    st.write('df tr∆∞·ªõc khi lo·∫°i outliers')
    st.dataframe(RFM[['Recency','Frequency','Monetary']].describe())
    st.write('df sau khi lo·∫°i outliers')
    st.dataframe(new_df[['Recency','Frequency','Monetary']].describe())
    # plot the distribution of RFM values after remove outliers
    st.write('Bi·ªÉu ƒë·ªì distplot sau khi lo·∫°i b·ªè outliers')
    f,ax = plt.subplots(figsize=(10, 12))
    plt.subplot(3, 1, 1)
    sns.distplot(new_df.Recency, label = 'Recency')
    plt.subplot(3, 1, 2)
    sns.distplot(new_df.Frequency, label = 'Frequency')
    plt.subplot(3, 1, 3)
    sns.distplot(new_df.Monetary, label = 'Monetary Value')
    st.pyplot(f)
    st.write('''
    Nh·∫≠n x√©t:
    Nh√¨n bi·ªÉu ƒë·ªì ta c√≥ th·ªÉ th·∫•y bi·ªÉu ƒë·ªì ph√¢n ph·ªëi kh√¥ng thay ƒë·ªïi m·∫•y sau khi lo·∫°i b·ªè c√°c outliers
    ''')
    st.code('hopkins test' + ' ' + str(hopkins(new_df, new_df.shape[0])) )
    st.write('''
    Nh·∫≠n x√©t:
    hopkins test cho s·ªë k·∫øt qu·∫£ ti·ªám c·∫≠n 0 =>>> c√≥ th·ªÉ ph√¢n c·ª•m t·ªët
    ''')
    st.subheader('Scale data & PCA')
    st.dataframe(df_scale)
    f, ax = plt.subplots(figsize=(8, 5))
    plt.plot(ks, inertias, '-o')
    plt.xlabel('Number of clusters, k')
    plt.ylabel('Inertia')
    plt.xticks(ks)
    plt.style.use('ggplot')
    plt.title('ƒê√¢u l√† k t·ªët nh·∫•t cho thu·∫≠t to√°n KMeans ?')
    st.plotly_chart(f)
    st.write('=>> Theo K-Means Elbow Method ta ch·ªçn K = 3')
    st.write('')



#---------------------------------------------
# GUI
#---------------------------------------------
st.set_page_config(
    page_title="Capstone Project", page_icon="üìà", initial_sidebar_state="expanded"
)

st.title("Trung T√¢m Tin H·ªçc")
st.write("## Capstone Project - ƒê·ªì √°n t·ªët nghi·ªáp Data Science")

st.sidebar.header('Capstone Project')
st.sidebar.subheader("Customer Segmetation s·ª≠ d·ª•ng RFM")

menu=['GI·ªöI THI·ªÜU','TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU','K·∫æT QU·∫¢ PH√ÇN C·ª§M']
choice=st.sidebar.selectbox('Menu',menu)
st.sidebar.markdown(
        "<hr />",
        unsafe_allow_html=True
    )
if choice == 'GI·ªöI THI·ªÜU':
    st.markdown("<h1 style='text-align: center; color: white;'>Customer Segmentation</h1>", unsafe_allow_html=True) 
    st.write('''
     ''')
    st.subheader("Customer Segmentation l√† g√¨?")
    st.image("RFM_Model_1.png")
    st.image("RFM_Model_2.png")
    st.write('''
        Ph√¢n kh√∫c/nh√≥m/c·ª•m kh√°ch h√†ng (market segmentation ‚Äì c√≤n ƒë∆∞·ª£c g·ªçi l√† ph√¢n kh√∫c th·ªã tr∆∞·ªùng) l√† qu√° tr√¨nh
    nh√≥m c√°c kh√°ch h√†ng l·∫°i v·ªõi nhau d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm chung. N√≥ ph√¢n chia v√† nh√≥m kh√°ch h√†ng th√†nh c√°c nh√≥m 
    nh·ªè theo ƒë·∫∑c ƒëi·ªÉm ƒë·ªãa l√Ω, nh√¢n kh·∫©u h·ªçc, t√¢m l√Ω h·ªçc, h√†nh vi (geographic, demographic, psychographic, behavioral)
    v√† c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c.''')
    st.write('''
    C√°c nh√† ti·∫øp th·ªã s·ª≠ d·ª•ng k·ªπ thu·∫≠t n√†y ƒë·ªÉ nh·∫Øm m·ª•c ti√™u kh√°ch h√†ng th√¥ng qua vi·ªác c√° nh√¢n h√≥a, khi h·ªç mu·ªën
    tung ra c√°c chi·∫øn d·ªãch qu·∫£ng c√°o, truy·ªÅn th√¥ng, thi·∫øt k·∫ø m·ªôt ∆∞u ƒë√£i ho·∫∑c khuy·∫øn m√£i m·ªõi, v√† c≈©ng ƒë·ªÉ b√°n h√†ng.
    ''')
    st.subheader("Customer Segmetation s·ª≠ d·ª•ng RFM?")
    st.image("RFM_hinh1.png")
    st.write('''
    Ph√¢n t√≠ch RFM (Recency, Frequency, Monetary) l√† ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n d·ª±a tr√™n h√†nh vi c·ªßa kh√°ch
    h√†ng th√†nh ƒë·ªÉ nh√≥m th√†nh c√°c ph√¢n kh√∫c. RFM ph√¢n nh√≥m kh√°ch h√†ng tr√™n c∆° s·ªü c√°c giao d·ªãch mua h√†ng
    tr∆∞·ªõc ƒë√≥ c·ªßa h·ªç, nh·∫±m m·ª•c ƒë√≠ch ph·ª•c v·ª• kh√°ch h√†ng t·ªët h∆°n.
    ''')
    st.write('''
    RFM gi√∫p ng∆∞·ªùi qu·∫£n l√Ω x√°c ƒë·ªãnh ƒë∆∞·ª£c kh√°ch h√†ng ti·ªÅm nƒÉng ƒë·ªÉ kinh doanh c√≥ l·ª£i nhu·∫≠n h∆°n. Ngo√†i
    ra, n√≥ gi√∫p c√°c nh√† qu·∫£n l√Ω th·ª±c hi·ªán c√°c chi·∫øn d·ªãch qu·∫£ng c√°o hi·ªáu qu·∫£ cho d·ªãch v·ª• ƒë∆∞·ª£c c√° nh√¢n h√≥a.
    ''')
    st.write('''
    RFM nghi√™n c·ª©u h√†nh vi c·ªßa kh√°ch h√†ng v√† ph√¢n nh√≥m d·ª±a tr√™n ba y·∫øu t·ªë ƒëo l∆∞·ªùng:
    - Recency (R): ƒëo l∆∞·ªùng s·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua h√†ng cu·ªëi c√πng (l·∫ßn truy c·∫≠p g·∫ßn ƒë√¢y nh·∫•t) ƒë·∫øn ng√†y gi·∫£ ƒë·ªãnh chung
    ƒë·ªÉ t√≠nh to√°n (v√≠ d·ª•: ng√†y hi·ªán t·∫°i, ho·∫∑c ng√†y max trong danh s√°ch giao d·ªãch).
    - Frequency (F): ƒëo l∆∞·ªùng s·ªë l∆∞·ª£ng giao d·ªãch (t·ªïng s·ªë l·∫ßn mua h√†ng) ƒë∆∞·ª£c th·ª±c hi·ªán trong th·ªùi gian nghi√™n c·ª©u.
    - Monetary Value (M): ƒëo l∆∞·ªùng s·ªë ti·ªÅn m√† m·ªói kh√°ch h√†ng ƒë√£ chi ti√™u trong th·ªùi gian nghi√™n c·ª©u.
    ''')

    st.subheader('Business Objective/Problem')
    st.write('''
    C√¥ng ty X ch·ªß y·∫øu ph√°t h√†nh v√† b√°n s·∫£n ph·∫©m l√† CD t·ªõi kh√°ch h√†ng.
    C√¥ng ty X mong mu·ªën c√≥ th·ªÉ b√°n ƒë∆∞·ª£c nhi·ªÅu s·∫£n ph·∫©m h∆°n c≈©ng nh∆∞ gi·ªõi
    thi·ªáu s·∫£n ph·∫©m ƒë·∫øn ƒë√∫ng ƒë·ªëi t∆∞·ª£ng, chƒÉm s√≥c v√† l√†m h√†i l√≤ng kh√°ch h√†ng.
    Data set CDNOW_master.txt ch·ª©a to√†n b·ªô l·ªãch s·ª≠ mua h√†ng t·ª´ qu√Ω ƒë·∫ßu ti√™n 
    nƒÉm 1997 cho ƒë·∫øn h·∫øt qu√Ω th·ª© hai nƒÉm 1998 (cu·ªëi th√°ng 06/1998) 
    c·ªßa 23.570 kh√°ch h√†ng ƒë√£ th·ª±c hi·ªán.
    ''')
elif choice =='TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU':
    run_all(rfm_df)
    
elif choice=='K·∫æT QU·∫¢ PH√ÇN C·ª§M':
    st.write('Trung b√¨nh gi√° tr·ªã RFM v√† s·ªë l∆∞·ª£ng kh√°ch h√†ng ·ªü m·ªói nh√≥m ƒë∆∞·ª£c ph√¢n c·ª•m')
    st.dataframe(km_pca)
    st.code('silhouette score' + ' ' + str(silhouette_score(PCA_components.iloc[:,:1], kmean1.labels_, metric='euclidean')))
    st.write('''
    Nh·∫≠n x√©t:
    silhouette score cho gi√° tr·ªã g·∫ßn b·∫±ng 1, c√≥ th·ªÉ th·∫•y k·∫øt qu·∫£ kh√° t·ªët.
    ''')
    st.subheader('Tr·ª±c quan h√≥a k·∫øt qu·∫£ ƒë∆∞·ª£c ph√¢n c·ª•m')
    fig = px.scatter_3d(rfm_rfm_k, x="Recency", y="Monetary", z="Frequency",
                    color = 'K_Cluster', width=800, height=400)
    st.plotly_chart(fig)
    st.write('''
    D·ª±a v√†o k·∫øt qu·∫£ ta c√≥ th·ªÉ ƒë·∫∑t t√™n cho c√°c c·ª•m kh√°ch h√†ng nh∆∞ sau:
    
    0. GOLD
    
    1. HIDDEN GEM
    
    2. STAR
    ''')
    st.write('')
    st.write('')
    st.write('t√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c√°c RFM_Level v√† t·ª∑ tr·ªçng c·ªßa c√°c Segments')
    st.dataframe(rfm_agg2)
    st.image('Kmean_Segments.png', caption = 'Unsupervised Segments')
